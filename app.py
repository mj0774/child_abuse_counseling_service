from flask import Flask, request, jsonify, render_template
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np
import json
from datetime import datetime
import os
import uuid

app = Flask(__name__)

# AI ëª¨ë¸ ë¡œë“œ
try:
    MODEL_PATH = "mjhwang/roberta-reg-eval"
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
    model.eval()
    print("AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    IS_AI_MODEL_LOADED = True
except Exception as e:
    print(f"AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    model = None
    tokenizer = None
    IS_AI_MODEL_LOADED = False


@app.route("/")
def home():
    return render_template("main.html")

@app.route("/question")
def question():
    return render_template("question.html")

@app.route("/result")
def result():
    return render_template("result.html")

@app.route("/feedback")
def feedback():
    return render_template("feedback.html")


@app.route('/api/submit-assessment', methods=['POST'])
def submit_assessment():
    try:
        raw_data = request.get_json()
        
        if not raw_data:
            return jsonify({
                'success': False, 
                'message': 'ë°ì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'
            }), 400

        processed_data = process_assessment_data(raw_data)
        analysis_result = request_ai_analysis(processed_data)
        save_assessment_result(raw_data, analysis_result)

        return jsonify({
            'success': True,
            'analysis': analysis_result,
            'message': 'í‰ê°€ ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.'
        }), 200

    except Exception as e:
        print(f"í‰ê°€ ì œì¶œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return jsonify({
            'success': False,
            'message': f"ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }), 500


def request_ai_analysis(processed_data):
    if IS_AI_MODEL_LOADED:
        return analyze_with_ai(processed_data)
    else:
        return generate_dummy_analysis(processed_data)


def analyze_with_ai(processed_data):
    """ì‹¤ì œ AI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì˜ ìœ„í—˜ë„ë¥¼ ë¶„ì„"""
    try:
        answers_dict = processed_data['answers']
        
        combined_texts = [
            f"ì§ˆë¬¸: {item['question_text']} ë‹µë³€: {item['answer_text']}" 
            for item in answers_dict.values()
        ]

        if not combined_texts:
            print("ë¶„ì„í•  ë‹µë³€ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            return generate_dummy_analysis(processed_data)

        # ğŸ’¡ ìˆ˜ì •: combined_textsë¥¼ í† í¬ë‚˜ì´ì €ì— ì „ë‹¬í•©ë‹ˆë‹¤.
        inputs = tokenizer(
            combined_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        with torch.no_grad():
            outputs = model(**inputs)
            # 'tuple' object has no attribute 'logits' ì˜¤ë¥˜ í•´ê²° -> ê·¼ë° ë¡œì§“ê°’ì´ ê³„ì† ë™ì¼í•˜ê²Œ ë‚˜ì˜´ 
            predicted_scores = torch.sigmoid(outputs[0]).squeeze().tolist()
        
        # ëª¨ë¸ ì¶œë ¥ì´ ë‹¨ì¼ ê°’ì¼ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if not isinstance(predicted_scores, list):
            predicted_scores = [predicted_scores]

        analysis_scores = []
        for i, question_id in enumerate(answers_dict.keys()):
            # ì ìˆ˜ë¥¼ 0-100 ëŒ€ì‹  0-10ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§
            score = 10 * predicted_scores[i]
            
            # processed_dataì—ì„œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì°¾ê¸°
            main_category = "ë¶„ì„ ê²°ê³¼"
            sub_category = "ì•Œ ìˆ˜ ì—†ìŒ"
            for category_id, category_info in processed_data['categories'].items():
                if question_id in category_info['questions']:
                    try:
                        main_category = category_info['title'].split(' - ')[0].strip()
                        sub_category = category_info['title'].split(' - ')[-1].strip()
                    except IndexError:
                        main_category = category_info['title']
                    break

            analysis_scores.append({
                "mainCategory": main_category,
                "subCategory": sub_category,
                "question": answers_dict[question_id]['question_text'],
                "userAnswer": answers_dict[question_id]['answer_text'], 
                "score": round(score, 2)
            })

        average_score = sum(d['score'] for d in analysis_scores) / len(analysis_scores) if analysis_scores else 0

        if average_score >= 8:
            risk_level = "í•™ëŒ€ì˜ì‹¬"
        elif average_score >= 6:
            risk_level = "ìƒë‹´í•„ìš”"
        elif average_score >= 4:
            risk_level = "ê´€ì°°í•„ìš”"
        else:
            risk_level = "ì •ìƒêµ°"
        
        findings = ["AI ëª¨ë¸ì´ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ê²°ê³¼ë¥¼ ë„ì¶œí–ˆìŠµë‹ˆë‹¤."]
        recommendations = ["ëª¨ë¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì¶”ê°€ì ì¸ ìƒë‹´ì„ ê³ ë ¤í•´ë³´ì„¸ìš”."]
        
        return {
            'scores': analysis_scores,
            'averageScore': round(average_score, 2),
            'riskLevel': risk_level,
            'findings': findings,
            'recommendations': recommendations
        }
    except Exception as e:
        print(f"AI ëª¨ë¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return generate_dummy_analysis(processed_data)


def process_assessment_data(data):
    """
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë°›ì€ ë°ì´í„°ë¥¼ ë¶„ì„ì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜.
    ğŸ’¡ ì§ˆë¬¸ í…ìŠ¤íŠ¸ì™€ ë‹µë³€ì„ í•¨ê»˜ ì €ì¥í•˜ë„ë¡ ìˆ˜ì •.
    """
    processed_data = {
        'answered_questions': data['summary']['answeredQuestions'],
        'total_questions': data['summary']['totalQuestions'],
        'categories': {c['id']: {'title': c['title'], 'questions': [q['questionId'] for q in c['answers']]} for c in data['summary']['categories']},
        'answers': {q['questionId']: {'answer_text': q['answer'], 'question_text': q['question']} for c in data['summary']['categories'] for q in c['answers'] if q['answer']}
    }
    return processed_data


#def generate_dummy_analysis(processed_data):
    # AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©ë˜ëŠ” ë”ë¯¸ ë¡œì§
    dummy_scores = []
    total_score = 0
    answered_count = 0

    for category_id, category_info in processed_data['categories'].items():
        for question_id in category_info['questions']:
            if question_id in processed_data['answers']:
                answered_count += 1
                answer_length = len(processed_data['answers'][question_id]['answer_text'])
                score = min(10.0, 1.0 + (answer_length - 10) / 90 * 9.0)
                
                dummy_scores.append({
                    "mainCategory": category_info['title'].split(' - ')[0].strip(),
                    "subCategory": category_info['title'].split(' - ')[-1].strip(),
                    "question": processed_data['answers'][question_id]['question_text'],
                    "score": round(score, 2)
                })

    average_score = sum(d['score'] for d in dummy_scores) / len(dummy_scores) if dummy_scores else 0
    
    if average_score >= 8:
        risk_level = "í•™ëŒ€ì˜ì‹¬"
    elif average_score >= 6:
        risk_level = "ìƒë‹´í•„ìš”"
    elif average_score >= 4:
        risk_level = "ê´€ì°°í•„ìš”"
    else:
        risk_level = "ì •ìƒêµ°"
    
    findings = [
        f"ì´ {processed_data.get('total_questions', 0)}ê°œ ì§ˆë¬¸ ì¤‘ {answered_count}ê°œì— ë‹µë³€í•˜ì…¨ìŠµë‹ˆë‹¤.",
        "ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ì‹œ í‰ê°€ë¥¼ ì‹¤ì‹œí–ˆìŠµë‹ˆë‹¤."
    ]
    recommendations = [
        "AI ëª¨ë¸ì´ ì—°ê²°ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê±°ë‚˜, ê°œë°œìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.",
        "ë‹µë³€ì´ ê¸¸ìˆ˜ë¡ ë†’ì€ ìœ„í—˜ë„ë¥¼ ë³´ì…ë‹ˆë‹¤. ì¶”ê°€ì ì¸ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    ]
    
    return {
        'scores': dummy_scores,
        'averageScore': round(average_score, 2),
        'riskLevel': risk_level,
        'findings': findings,
        'recommendations': recommendations
    }


def generate_submission_id():
    return str(uuid.uuid4())

def save_assessment_result(assessment_data, analysis_result):
    try:
        if not os.path.exists('results'):
            os.makedirs('results')
        
        save_data = {
            'timestamp': datetime.now().isoformat(),
            'assessment': assessment_data,
            'analysis': analysis_result
        }
        
        filename = f"assessment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = os.path.join('results', filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"í‰ê°€ ê²°ê³¼ ì €ì¥ë¨: {filepath}")
    except Exception as e:
        print(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == '__main__':
    app.run(debug=True)