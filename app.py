from flask import Flask, request, jsonify, render_template
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np
import json
from datetime import datetime
import os
import uuid

app = Flask(__name__)

# AI ëª¨ë¸ ë¡œë“œ
try:
    MODEL_PATH = "mjhwang/roberta-reg-eval"
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
    model.eval()
    print("AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    IS_AI_MODEL_LOADED = True
except Exception as e:
    print(f"AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    model = None
    tokenizer = None
    IS_AI_MODEL_LOADED = False


@app.route("/")
def home():
    return render_template("main.html")

@app.route("/question")
def question():
    return render_template("question.html")

@app.route("/result")
def result():
    return render_template("result.html")

@app.route("/feedback")
def feedback():
    return render_template("feedback.html")


@app.route('/api/submit-assessment', methods=['POST'])
def submit_assessment():
    try:
        raw_data = request.get_json()
        
        if not raw_data:
            return jsonify({
                'success': False, 
                'message': 'ë°ì´í„°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'
            }), 400

        processed_data = process_assessment_data(raw_data)
        analysis_result = request_ai_analysis(processed_data)
        save_assessment_result(raw_data, analysis_result)

        return jsonify({
            'success': True,
            'analysis': analysis_result,
            'message': 'í‰ê°€ ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤.'
        }), 200

    except Exception as e:
        print(f"í‰ê°€ ì œì¶œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return jsonify({
            'success': False,
            'message': f"ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        }), 500


def request_ai_analysis(processed_data):
    if IS_AI_MODEL_LOADED:
        return analyze_with_ai(processed_data)
    else:
        return generate_dummy_analysis(processed_data)


def analyze_with_ai(processed_data):
    try:
        texts, metas = [], []
        for cat_info in processed_data["categories"].values():
            main_title, sub_title = map(str.strip, cat_info["title"].split(" - ", 1))

            qa_list = []
            answers = []
            for qid in cat_info["questions"]:
                qa = processed_data["answers"].get(qid, {})
                q, a = qa.get("question_text", ""), qa.get("answer_text", "")
                qa_list.append({"question": q, "answer": a})
                if a:
                    answers.append(a.strip())

            joined = " ".join(answers) if answers else None
            if joined:
                texts.append(f"[TYPE] {sub_title} [A] {joined}")
            metas.append((main_title, sub_title, qa_list, joined))

        # ëª¨ë¸ ì ìˆ˜ ì˜ˆì¸¡ (í…ìŠ¤íŠ¸ í•˜ë‚˜ì”© ë„£ê¸°)
        scores = []
        for t in texts:
            inputs = tokenizer(t, return_tensors="pt", padding=True,
                               truncation=True, max_length=512)
            with torch.no_grad():
                # outputs = model(**inputs)
                out = model(**inputs, return_dict=True)
                logits = out.logits if hasattr(out, "logits") else out[0]
                y = logits.squeeze(-1).item()
                score = float(np.clip(y, 0, 10))
            # logits = getattr(outputs, "logits", outputs[0])
            # score = (torch.sigmoid(logits).view(-1).item() * 10)
            scores.append(score)

        # ê²°ê³¼ ì¡°ë¦½
        results, idx, valid_scores = [], 0, []
        for main_title, sub_title, qa_list, joined in metas:
            score = 0.0
            if joined:
                score = round(scores[idx], 2)
                valid_scores.append(score)
                idx += 1

            if score >= 8: level = "í•™ëŒ€ì˜ì‹¬"
            elif score >= 6: level = "ìƒë‹´í•„ìš”"
            elif score >= 4: level = "ê´€ì°°í•„ìš”"
            else: level = "ì •ìƒêµ°"

            results.append({
                "mainCategory": main_title,
                "subCategory": sub_title,
                "score": score,
                "questions": qa_list,
                "riskLevel": level
            })

        # í‰ê· ì€ ë‹µë³€ ìˆëŠ” í•­ëª©ë§Œ í¬í•¨
        avg = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0
        if avg >= 8: overall_level = "í•™ëŒ€ì˜ì‹¬"
        elif avg >= 6: overall_level = "ìƒë‹´í•„ìš”"
        elif avg >= 4: overall_level = "ê´€ì°°í•„ìš”"
        else: overall_level = "ì •ìƒêµ°"

        return {
            "scores": results,
            "averageScore": round(avg, 2),
            "riskLevel": overall_level
        }

    except Exception as e:
        print("ì˜¤ë¥˜:", e)
        return generate_dummy_analysis(processed_data)





def process_assessment_data(data):
    """
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ë°›ì€ ë°ì´í„°ë¥¼ ë¶„ì„ì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜.
    ğŸ’¡ ì§ˆë¬¸ í…ìŠ¤íŠ¸ì™€ ë‹µë³€ì„ í•¨ê»˜ ì €ì¥í•˜ë„ë¡ ìˆ˜ì •.
    """
    processed_data = {
        'answered_questions': data['summary']['answeredQuestions'],
        'total_questions': data['summary']['totalQuestions'],
        'categories': {c['id']: {'title': c['title'], 'questions': [q['questionId'] for q in c['answers']]} for c in data['summary']['categories']},
        'answers': {q['questionId']: {'answer_text': q['answer'], 'question_text': q['question']} for c in data['summary']['categories'] for q in c['answers'] if q['answer']}
    }
    return processed_data


#def generate_dummy_analysis(processed_data):
    # AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©ë˜ëŠ” ë”ë¯¸ ë¡œì§
    dummy_scores = []
    total_score = 0
    answered_count = 0

    for category_id, category_info in processed_data['categories'].items():
        for question_id in category_info['questions']:
            if question_id in processed_data['answers']:
                answered_count += 1
                answer_length = len(processed_data['answers'][question_id]['answer_text'])
                score = min(10.0, 1.0 + (answer_length - 10) / 90 * 9.0)
                
                dummy_scores.append({
                    "mainCategory": category_info['title'].split(' - ')[0].strip(),
                    "subCategory": category_info['title'].split(' - ')[-1].strip(),
                    "question": processed_data['answers'][question_id]['question_text'],
                    "score": round(score, 2)
                })

    average_score = sum(d['score'] for d in dummy_scores) / len(dummy_scores) if dummy_scores else 0
    
    if average_score >= 8:
        risk_level = "í•™ëŒ€ì˜ì‹¬"
    elif average_score >= 6:
        risk_level = "ìƒë‹´í•„ìš”"
    elif average_score >= 4:
        risk_level = "ê´€ì°°í•„ìš”"
    else:
        risk_level = "ì •ìƒêµ°"
    
    findings = [
        f"ì´ {processed_data.get('total_questions', 0)}ê°œ ì§ˆë¬¸ ì¤‘ {answered_count}ê°œì— ë‹µë³€í•˜ì…¨ìŠµë‹ˆë‹¤.",
        "ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ì‹œ í‰ê°€ë¥¼ ì‹¤ì‹œí–ˆìŠµë‹ˆë‹¤."
    ]
    recommendations = [
        "AI ëª¨ë¸ì´ ì—°ê²°ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê±°ë‚˜, ê°œë°œìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.",
        "ë‹µë³€ì´ ê¸¸ìˆ˜ë¡ ë†’ì€ ìœ„í—˜ë„ë¥¼ ë³´ì…ë‹ˆë‹¤. ì¶”ê°€ì ì¸ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    ]
    
    return {
        'scores': dummy_scores,
        'averageScore': round(average_score, 2),
        'riskLevel': risk_level,
        'findings': findings,
        'recommendations': recommendations
    }


def generate_submission_id():
    return str(uuid.uuid4())

def save_assessment_result(assessment_data, analysis_result):
    try:
        if not os.path.exists('results'):
            os.makedirs('results')
        
        save_data = {
            'timestamp': datetime.now().isoformat(),
            'assessment': assessment_data,
            'analysis': analysis_result
        }
        
        filename = f"assessment_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = os.path.join('results', filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"í‰ê°€ ê²°ê³¼ ì €ì¥ë¨: {filepath}")
    except Exception as e:
        print(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == '__main__':
    app.run(debug=True)